{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../head.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from hic_basic.binnify import GenomeIdeograph\n",
    "from hic_basic.coolstuff import cli_balance, cli_compartment\n",
    "from hic_basic.sequence import count_CpG\n",
    "from hic_basic.hicio import read_meta, load_json, dump_json\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from lib.plot import plot_2_scAB\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from hic_basic.hicio import load_json\n",
    "from lib.metrics import stack_contour\n",
    "from lib.plot import plot_heatmap_with_bars, plot_figure_canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_scAB(file, region, kregion, keyfunc):\n",
    "    \"\"\"\n",
    "    Note:\n",
    "      Will drop rows with all NaNs in kregion.\n",
    "    \"\"\"\n",
    "    data = pd.read_parquet(file)\n",
    "    data.sort_index(inplace=True)\n",
    "    if isinstance(region, str):\n",
    "        # whole chrom\n",
    "        chrom = region\n",
    "        data = data.loc[\n",
    "            (chrom,) : (chrom,)\n",
    "            ].droplevel(0).T\n",
    "    else: \n",
    "        chrom, start, end = region\n",
    "        data = data.loc[\n",
    "            (chrom,start) : (chrom,end)\n",
    "            ].droplevel(0).T\n",
    "    target_data = pd.read_parquet(file)\n",
    "    target_data.sort_index(inplace=True)\n",
    "    kchrom, kstart, kend = kregion\n",
    "    target_data = target_data.loc[\n",
    "        (kchrom,kstart) : (kchrom,kend)\n",
    "    ].droplevel(0).T\n",
    "    sorted_samples = target_data.dropna(how=\"all\").apply(\n",
    "        keyfunc,\n",
    "        axis=1\n",
    "    ).sort_values().index.tolist()\n",
    "    return data.loc[sorted_samples]\n",
    "def chunk_sum(chunk, cols, sample_dict):\n",
    "    \"\"\"\n",
    "    Select col and rows according to sample_dict repeatedly and sum.\n",
    "    Input:\n",
    "        chunk: pd.DataFrame, the chunk of the bfs file.\n",
    "        cols: list, columns to be aggregated.\n",
    "        sample_dict: dict, {agg_feature_name: [samples]}.\n",
    "    Output:\n",
    "        df: pd.DataFrame, aggregated features.\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    for col in cols:\n",
    "        samples = sample_dict[col]\n",
    "        df = chunk.loc[ # only part of the samples will appear in the chunk\n",
    "            chunk.index.get_level_values(0).isin(samples), col\n",
    "            ].groupby(level=[1,2]).sum() # groupby ht and dv\n",
    "        df_list.append(df)\n",
    "    df = pd.concat(df_list, axis=1)\n",
    "    return df.sum(axis=1)\n",
    "#chunk_sum(chunk, list(sample_dict.keys()), sample_dict).sum()\n",
    "def scAB_feature_agg(bfs_file, sample_dict, chunksize=50000, report=False):\n",
    "    \"\"\"\n",
    "    Aggregating scAB features.\n",
    "    Input:\n",
    "        bfs_file: str, path to the bfs h5 file.\n",
    "        sample_dict: dict, {agg_feature_name: [samples]}.\n",
    "        chunksize: int, chunk size for reading the bfs file.\n",
    "        report: bool, print the progress.\n",
    "    Output:\n",
    "        dist: pd.Series, aggregated features.\n",
    "    \"\"\"\n",
    "    col_list = list(sample_dict.keys())\n",
    "    with pd.HDFStore(bfs_file,\"r\") as store:\n",
    "        chunk_sum_list = []\n",
    "        for chunk in store.select(\n",
    "                \"main\",\n",
    "                where = 'lr == 0',\n",
    "                columns = [\"ht\",\"dv\",\"sample_name\"] + col_list,\n",
    "                iterator=True,\n",
    "                chunksize=chunksize\n",
    "                ):\n",
    "            chunk = chunk.set_index([\"sample_name\",\"ht\",\"dv\"])\n",
    "            # calculate complex feature like \"Astrong\" in the chunk\n",
    "            chunk_sum_res = chunk_sum(\n",
    "                chunk, col_list, sample_dict\n",
    "                ).rename(\"count\").reset_index([\"ht\",\"dv\"])\n",
    "            chunk_sum_list.append(\n",
    "                chunk_sum_res\n",
    "            )\n",
    "            if report:\n",
    "                print(\"add\",chunk_sum_res[\"count\"].sum().sum())\n",
    "    dist = pd.concat(\n",
    "        chunk_sum_list,axis=0 # sum all samples\n",
    "        ).groupby([\"ht\",\"dv\"])[\"count\"].sum()\n",
    "    return dist\n",
    "# data = scAB_feature_agg(\n",
    "#     arg_bfs_vx_files[\"Sperm_scAB\"],\n",
    "#     Bstrong\n",
    "#     )\n",
    "def get_density(bfs_file):\n",
    "    # just use density of lr==0 to normalize the data\n",
    "    # won't give a meaningful frequency by dividing the density\n",
    "    with pd.HDFStore(bfs_file, \"r\") as store:\n",
    "        density = store.select(\n",
    "            \"main\",\n",
    "            where=\"lr == 0\",\n",
    "            columns=[\"ht\",\"dv\",\"density\"]\n",
    "            )\n",
    "    density.loc[density[\"density\"]==0] = pd.NA\n",
    "    density = density.groupby([\"ht\",\"dv\"])[\"density\"].mean()\n",
    "    return density\n",
    "# density = get_density(arg_bfs_vx_files[\"Sperm\"])\n",
    "# density[density.rank(method=\"dense\").astype(int) == 3]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
